{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.Insurance_Fraud.logger.logger import logger\n",
    "from src.Insurance_Fraud.constants import *\n",
    "from src.Insurance_Fraud.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_bind_date</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_csl</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>...</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>auto_model</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>fraud_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>OH</td>\n",
       "      <td>250/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>466132</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>71610</td>\n",
       "      <td>6510</td>\n",
       "      <td>13020</td>\n",
       "      <td>52080</td>\n",
       "      <td>Saab</td>\n",
       "      <td>92x</td>\n",
       "      <td>2004</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>42</td>\n",
       "      <td>342868</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>IN</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1197.22</td>\n",
       "      <td>5000000</td>\n",
       "      <td>468176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>5070</td>\n",
       "      <td>780</td>\n",
       "      <td>780</td>\n",
       "      <td>3510</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>E400</td>\n",
       "      <td>2007</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>2000-09-06</td>\n",
       "      <td>OH</td>\n",
       "      <td>100/300</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>430632</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>NO</td>\n",
       "      <td>34650</td>\n",
       "      <td>7700</td>\n",
       "      <td>3850</td>\n",
       "      <td>23100</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2007</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>1990-05-25</td>\n",
       "      <td>IL</td>\n",
       "      <td>250/500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>608117</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>63400</td>\n",
       "      <td>6340</td>\n",
       "      <td>6340</td>\n",
       "      <td>50720</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Tahoe</td>\n",
       "      <td>2014</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>IL</td>\n",
       "      <td>500/1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>610706</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>6500</td>\n",
       "      <td>1300</td>\n",
       "      <td>650</td>\n",
       "      <td>4550</td>\n",
       "      <td>Accura</td>\n",
       "      <td>RSX</td>\n",
       "      <td>2009</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   months_as_customer  age  policy_number policy_bind_date policy_state  \\\n",
       "0                 328   48         521585       2014-10-17           OH   \n",
       "1                 228   42         342868       2006-06-27           IN   \n",
       "2                 134   29         687698       2000-09-06           OH   \n",
       "3                 256   41         227811       1990-05-25           IL   \n",
       "4                 228   44         367455       2014-06-06           IL   \n",
       "\n",
       "  policy_csl  policy_deductable  policy_annual_premium  umbrella_limit  \\\n",
       "0    250/500               1000                1406.91               0   \n",
       "1    250/500               2000                1197.22         5000000   \n",
       "2    100/300               2000                1413.14         5000000   \n",
       "3    250/500               2000                1415.74         6000000   \n",
       "4   500/1000               1000                1583.91         6000000   \n",
       "\n",
       "   insured_zip  ... witnesses police_report_available total_claim_amount  \\\n",
       "0       466132  ...         2                     YES              71610   \n",
       "1       468176  ...         0                       ?               5070   \n",
       "2       430632  ...         3                      NO              34650   \n",
       "3       608117  ...         2                      NO              63400   \n",
       "4       610706  ...         1                      NO               6500   \n",
       "\n",
       "  injury_claim property_claim  vehicle_claim  auto_make auto_model auto_year  \\\n",
       "0         6510          13020          52080       Saab        92x      2004   \n",
       "1          780            780           3510   Mercedes       E400      2007   \n",
       "2         7700           3850          23100      Dodge        RAM      2007   \n",
       "3         6340           6340          50720  Chevrolet      Tahoe      2014   \n",
       "4         1300            650           4550     Accura        RSX      2009   \n",
       "\n",
       "  fraud_reported  \n",
       "0              Y  \n",
       "1              Y  \n",
       "2              N  \n",
       "3              Y  \n",
       "4              N  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/artifacts/data_ingestion/insurance_claims.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 54)\n",
      "(200, 54)\n",
      "(800,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "df = data.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "for col in categorical_columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "for col in numerical_columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "df['loss_ratio'] = df['total_claim_amount'] / (df['policy_annual_premium'] * 12)\n",
    "df['profitability'] = (df['policy_annual_premium'] * 12) - df['total_claim_amount']\n",
    "\n",
    "df['vehicle_age'] = 2025 - df['auto_year']\n",
    "df.drop('auto_year', axis=1, inplace=True)\n",
    "\n",
    "df['incident_date'] = pd.to_datetime(df['incident_date'])\n",
    "df['incident_year'] = df['incident_date'].dt.year\n",
    "df['incident_month'] = df['incident_date'].dt.month\n",
    "df['incident_day'] = df['incident_date'].dt.day\n",
    "df.drop('incident_date',axis=1,inplace=True)\n",
    "\n",
    "df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors='coerce')\n",
    "df['policy_bind_year'] = df['policy_bind_date'].dt.year\n",
    "df['policy_bind_month'] = df['policy_bind_date'].dt.month\n",
    "df['policy_bind_day'] = df['policy_bind_date'].dt.day\n",
    "df.drop('policy_bind_date',axis=1,inplace=True) \n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['incident_type'],drop_first=True).astype(int)],axis=1)\n",
    "df.drop('incident_type',axis=1,inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['collision_type'],drop_first=True).astype(int)],axis=1)\n",
    "df.drop('collision_type',axis=1,inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['authorities_contacted'],drop_first=True).astype(int)],axis=1)\n",
    "df.drop('authorities_contacted',axis=1,inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['incident_severity'],drop_first=True).astype(int)],axis=1)\n",
    "df.drop('incident_severity',axis=1,inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['policy_csl'],drop_first=True).astype(int)],axis=1)\n",
    "df.drop('policy_csl',axis=1,inplace=True)\n",
    "\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 100], labels=['18-25', '26-35', '36-45', '46-55', '55+'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df[df.select_dtypes(include=['object']).columns] = df[df.select_dtypes(include=['object']).columns].apply(encoder.fit_transform)\n",
    "df['age_group'] = encoder.fit_transform(df['age_group'])\n",
    "\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['fraud_reported'] == 0]\n",
    "minority_class = df[df['fraud_reported'] == 1]\n",
    "\n",
    "# Perform undersampling on the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False, \n",
    "                                n_samples=500,  # Reduce to 500 samples\n",
    "                                random_state=42)\n",
    "\n",
    "# Perform oversampling on the minority class\n",
    "minority_oversampled = resample(minority_class, \n",
    "                                replace=True, \n",
    "                                n_samples=500,  # Increase to 500 samples\n",
    "                                random_state=42)\n",
    "\n",
    "# Combine the balanced classes\n",
    "balanced_df = pd.concat([majority_downsampled, minority_oversampled])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x = balanced_df.drop('fraud_reported', axis=1)\n",
    "y = balanced_df['fraud_reported']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Insurance_Fraud.constants import *\n",
    "from src.Insurance_Fraud.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path = Path(CONFIG_FILE_PATH),\n",
    "        params_file_path = Path(PARAMS_FILE_PATH),\n",
    "        schema_file_path = Path(SCHEMA_FILE_PATH)\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(Path(config_file_path))\n",
    "        self.params = read_yaml(Path(params_file_path))\n",
    "        self.schema = read_yaml(Path(schema_file_path))\n",
    "\n",
    "        self.config['data_validation']['unzip_data_dir'] = Path(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/artifacts/data_ingestion/Insurance_Claims.csv\")\n",
    "\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config['data_transformation']\n",
    "        create_directories([config['root_dir']])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config['root_dir'],\n",
    "            data_path=config['data_path']\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        df = pd.read_csv(\"C:\\\\Users\\\\Arpit Kadam\\\\Desktop\\\\Insurance-Fraud-Detection\\\\artifacts\\\\data_ingestion\\\\insurance_claims.csv\")\n",
    "\n",
    "        numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "        df.replace('?', np.nan, inplace=True)\n",
    "        for col in categorical_columns:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "        for col in numerical_columns:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "        df['loss_ratio'] = df['total_claim_amount'] / (df['policy_annual_premium'] * 12)\n",
    "        df['profitability'] = (df['policy_annual_premium'] * 12) - df['total_claim_amount']\n",
    "\n",
    "        df['vehicle_age'] = 2025 - df['auto_year']\n",
    "        df.drop('auto_year', axis=1, inplace=True)\n",
    "\n",
    "        df['incident_date'] = pd.to_datetime(df['incident_date'])\n",
    "        df['incident_year'] = df['incident_date'].dt.year\n",
    "        df['incident_month'] = df['incident_date'].dt.month\n",
    "        df['incident_day'] = df['incident_date'].dt.day\n",
    "        df.drop('incident_date',axis=1,inplace=True)\n",
    "\n",
    "        df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors='coerce')\n",
    "        df['policy_bind_year'] = df['policy_bind_date'].dt.year\n",
    "        df['policy_bind_month'] = df['policy_bind_date'].dt.month\n",
    "        df['policy_bind_day'] = df['policy_bind_date'].dt.day\n",
    "        df.drop('policy_bind_date',axis=1,inplace=True)        \n",
    "\n",
    "        df = pd.concat([df,pd.get_dummies(df['incident_type'],drop_first=True).astype(int)],axis=1)\n",
    "        df.drop('incident_type',axis=1,inplace=True)\n",
    "\n",
    "        df = pd.concat([df,pd.get_dummies(df['collision_type'],drop_first=True).astype(int)],axis=1)\n",
    "        df.drop('collision_type',axis=1,inplace=True)\n",
    "\n",
    "        df = pd.concat([df,pd.get_dummies(df['authorities_contacted'],drop_first=True).astype(int)],axis=1)\n",
    "        df.drop('authorities_contacted',axis=1,inplace=True)\n",
    "\n",
    "        df = pd.concat([df,pd.get_dummies(df['incident_severity'],drop_first=True).astype(int)],axis=1)\n",
    "        df.drop('incident_severity',axis=1,inplace=True)\n",
    "\n",
    "        df = pd.concat([df,pd.get_dummies(df['policy_csl'],drop_first=True).astype(int)],axis=1)\n",
    "        df.drop('policy_csl',axis=1,inplace=True)\n",
    "\n",
    "        df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 100], labels=['18-25', '26-35', '36-45', '46-55', '55+'])\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        df[df.select_dtypes(include=['object']).columns] = df[df.select_dtypes(include=['object']).columns].apply(encoder.fit_transform)\n",
    "        df['age_group'] = encoder.fit_transform(df['age_group'])\n",
    "\n",
    "\n",
    "        # Separate the majority and minority classes\n",
    "        majority_class = df[df['fraud_reported'] == 0]\n",
    "        minority_class = df[df['fraud_reported'] == 1]\n",
    "\n",
    "        # Perform undersampling on the majority class\n",
    "        majority_downsampled = resample(majority_class, \n",
    "                                        replace=False, \n",
    "                                        n_samples=500,  # Reduce to 500 samples\n",
    "                                        random_state=42)\n",
    "\n",
    "        # Perform oversampling on the minority class\n",
    "        minority_oversampled = resample(minority_class, \n",
    "                                        replace=True, \n",
    "                                        n_samples=500,  # Increase to 500 samples\n",
    "                                        random_state=42)\n",
    "\n",
    "        # Combine the balanced classes\n",
    "        balanced_df = pd.concat([majority_downsampled, minority_oversampled])\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        train, test = train_test_split(balanced_df, test_size=0.25, random_state=42)\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "\n",
    "        logger.info(f\"Data transformation completed successfully and saved to {self.config.root_dir}\")\n",
    "        logger.info(f\"Train data shape: {train.shape}\")\n",
    "        logger.info(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 00:32:08,440: INFO: common: Attempting to read YAML file from: C:\\Users\\Arpit Kadam\\Desktop\\Insurance-Fraud-Detection\\config\\config.yaml]\n",
      "[2025-01-22 00:32:08,448: INFO: common: Attempting to read YAML file from: C:\\Users\\Arpit Kadam\\Desktop\\Insurance-Fraud-Detection\\params.yaml]\n",
      "[2025-01-22 00:32:08,453: INFO: common: Attempting to read YAML file from: C:\\Users\\Arpit Kadam\\Desktop\\Insurance-Fraud-Detection\\schema.yaml]\n",
      "[2025-01-22 00:32:08,470: INFO: common: Directory created: artifacts]\n",
      "[2025-01-22 00:32:08,476: INFO: common: Directory created: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-22 00:32:08,645: INFO: 4132114995: Data transformation completed successfully and saved to artifacts/data_transformation]\n",
      "[2025-01-22 00:32:08,647: INFO: 4132114995: Train data shape: (750, 55)]\n",
      "[2025-01-22 00:32:08,649: INFO: 4132114995: Test data shape: (250, 55)]\n",
      "(750, 55)\n",
      "(250, 55)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager(config_file_path=Path(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/config/config.yaml\"),\n",
    "                                    params_file_path=Path(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/params.yaml\"),\n",
    "                                    schema_file_path=Path(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/schema.yaml\"))\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    try:\n",
    "        data_transformation.initiate_data_transformation()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Input data file not found. Please check the file path: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "Data Transformation Stage Completed\n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"C:/Users/Arpit Kadam/Desktop/Insurance-Fraud-Detection/artifacts/data_validation/STATUS.txt\"), \"r\") as f:\n",
    "    status = f.read().split(\"****************************************************\")[1].split(\" \")[1]\n",
    "\n",
    "print(status)\n",
    "# The issue is with the string comparison\n",
    "# Let's clean up the status string and do a proper comparison\n",
    "status = status.strip()  # Remove any whitespace\n",
    "if status == \"True\":\n",
    "    print(\"Data Transformation Stage Completed\")\n",
    "else:\n",
    "    print(\"Data Transformation Stage Failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
